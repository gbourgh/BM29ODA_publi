%------------------------------------------------------------------------------
% Template file for the submission of papers to IUCr journals in LaTeX2e
% using the iucr document class
% Copyright 1999-2013 International Union of Crystallography
% Version 1.6 (28 March 2013)
%------------------------------------------------------------------------------

\documentclass[preprint]{iucr}              % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % Information about journal to which submitted
     %-------------------------------------------------------------------------
     \journalcode{J}              % Indicate the journal to which submitted
                                  %   A - Acta Crystallographica Section A
                                  %   B - Acta Crystallographica Section B
                                  %   C - Acta Crystallographica Section C
                                  %   D - Acta Crystallographica Section D
                                  %   E - Acta Crystallographica Section E
                                  %   F - Acta Crystallographica Section F
                                  %   J - Journal of Applied Crystallography
                                  %   M - IUCrJ
                                  %   S - Journal of Synchrotron Radiation

\begin{document}                  % DO NOT DELETE THIS LINE

     %-------------------------------------------------------------------------
     % The introductory (header) part of the paper
     %-------------------------------------------------------------------------

     % The title of the paper. Use \shorttitle to indicate an abbreviated title
     % for use in running heads (you will need to uncomment it).

\title{Online data analysis at ESRF BiosSaxs beamline}
%\shorttitle{Short Title}

     % Authors' names and addresses. Use \cauthor for the main (contact) author.
     % Use \author for all other authors. Use \aff for authors' affiliations.
     % Use lower-case letters in square brackets to link authors to their
     % affiliations; if there is only one affiliation address, remove the [a].

\cauthor[a]{J\'er\^ome}{Kieffer}{jerome.kieffer@esrf.fr}{}
\author[b]{Adam}{Round}
\author[a]{Petra}{Pernot}
\author[a]{Martha}{Brennich}
\author[a]{Alejandro}{De Maria Antolinos}

\aff[a]{ESRF Grenoble TODO \country{France}}
\aff[b]{EMBL Grenoble TODO \country{France}}

     % Use \shortauthor to indicate an abbreviated author list for use in
     % running heads (you will need to uncomment it).

\shortauthor{Kieffer et all.}

     % Use \vita if required to give biographical details (for authors of
     % invited review papers only). Uncomment it.

%\vita{Author's biography}

     % Keywords (required for Journal of Synchrotron Radiation only)
     % Use the \keyword macro for each word or phrase, e.g. 
     % \keyword{X-ray diffraction}\keyword{muscle}

\keyword{Online data-analysis, solution scattering, protein}

     % PDB and NDB reference codes for structures referenced in the article and
     % deposited with the Protein Data Bank and Nucleic Acids Database (Acta
     % Crystallographica Section D). Repeat for each separate structure e.g
     % \PDBref[dethiobiotin synthetase]{1byi} \NDBref[d(G$_4$CGC$_4$)]{ad0002}

%\PDBref[optional name]{refcode}
%\NDBref[optional name]{refcode}

\maketitle                        % DO NOT DELETE THIS LINE

\begin{synopsis}
Low-latency data reduction and real-time feed back to the users 
\end{synopsis}

\begin{abstract}
Abstract+Introduction (with motivation) -> Adam
Saxs applies to proteins blabla
high throughput blabla
need automatic data treatement blabla
\end{abstract}


     %-------------------------------------------------------------------------
     % The main body of the paper
     %-------------------------------------------------------------------------
     % Now enter the text of the document in multiple \section's, \subsection's
     % and \subsubsection's as required.

\section{Introduction}

Adam ?
Text text text text text text text text text text text text text text
text text text text text text text.



\section{Online data analysis}

Online data analysis is needed on highly automated beamlines where the
acquisition of a sample lasts for less than a second and there is virtually no
dead-time between samples.


\subsection{experiment control}
The sequence of acquisition is controled from a graphical interface, BsxCube,
written in Python/Qt4 \cite{pyqt} which enforces a strong Model-View-Controler
pattern\cite{mvc} (MVC here after).
BsxCube is composed of Control-objects (the Model in MVC) and from Bricks (the
View in MVC) which communicate togeather and with the rest of the beamline
(detector, motors, sample changer, \ldots) via protocols like 0MQ\cite{zmq} or
tango\cite{tango} which are the Controler in the MVC pattern.

\subsection{Data analysis server}

Data analysis is triggered automatically by BsxCube via a tango call. 
The data analysis server is actually a tango device server written in
PyTango\cite{pytango} which launches EDNA jobs\cite{edna}.  


\subsubsection{EDNA} is a plugin-based framwork to build pipeline for
data-analysis. So the basic idea behind EDNA is to have plugins either responsible for the execution
of a task or an external process, called \textem{execution plugins} or
\textem{control plugins} which are in charge of launching and managing other
plugin (either execution or control plugins), either sequentially or in
parallel. 
The number of execution plugins running simultaneouly is limited,
depending on the processor resources of the computer.
Any plugin recieves its
arguments and returns its result using data-structures which can be serialized
into XML to be able to send them to disk or via the network. 

An EDNA job is basically a top level control plugin which has some additional
feature to be able to communicate with the outside world, providing its state,
or its results even after all processing are over, while keeping its memory
footprint as limited as possible. 

\subsubsection{The tango device server} starts the EDNA
jobs via the \textem{startJob} command. This command takes as parameter a 
plugin name and the input data-structure associated to it, and return a job
identifier the processing requester, here BsxCube.
This tango device server is hence completely generic and can be used for any
type of EDNA jobs.

To have the best responsiveness at the tango level, EDNA jobs are not started as
they arrive but they are just instanciated and queued.  
Another thread is
responsible to start them, starting multiple jobs in parallel
(data-parallelism). The number of jobs and the number of actual execution
plugins running simultaneously is controlled independently to do the best usage
of the computing power available within the  single computer.

Once finished, any EDNA job emits a tango-event to announce its success or its
failure. BsxCube can then retrieve the result of the processing, i.e. the 1D
integrated scattering curve and display it. 

\section{Data Acquisition using the sample changer}

The sample changer allows a very high sample throughput and in this mode there
is a alternance of buffer and sample diluted in the buffer. 

One can basically distinguish three kind of pipelines which are triggered for
every frame, for every run and for every sample, so with very different
eigen-period.

\subsection{Azimuthal integration pipeline}

This pipeline is triggered for every single frame acquired, hence every second
and needs to have the lowest latency possible to display the integrated curve
in \textem{real time} into the beamline control graphical interface BsxCube.
The input data-structure for azimuthal integration contains especially
information about the geometry of the experiment, the sample, its concentration
and the transmitted intensity as measured on the beam-stop diode for
normalization.
To cope with the requested speed (dozens of Hertz requested), this pipeline
has been optimized and simplified to the maximum. 
It relies on FabIO\cite{fabio} for image reading (from NFS mounted disks) and
pyFAI\cite{pyFAI} for azimuthal integration. 
The detector being a pixel detector (Pilatus 1M from Dectris), the error can be
assumed to be Poissonian and is integrated as such. 
On the BioSaxs beamline at ESRF, the azimuthal integration calculation is
performed on a dedicated graphics card using OpenCL, offloading this processing from CPU to
GPU, hence leaving resources to other pipelines. 
Subsequent normalization are
performed directly using numpy\cite{numpy}.
The result is saved into a 3-column ASCII file suitable for further processing
using the ATSAS tools\cite{atsas} and sent back to BsxCube for live display. 

\subsection{Curve merging}
Data acquisition is usually performed in runs and consists in a dozen of frames
of the same sample to be able to evaluate the radiation damage effects.
This pipeline, called \textem{smartMerge}, is triggered at the end of each run
and is responsible for evaluating the similarity of different scattering
patters, using the \textem{datcmp} tool from ATSAS and merging all frames
similar to the first and propagate the associated metadata. All curve comparison
can be performed in parallel on modern multicore computer.

Subsequently, the \textem{smartMerge} checks for the buffer-sample-buffer
sequence and performs the averaging of the buffers and the subtraction from the
sample data, providing the so called \textem{subtracted curve} containing only
the scattering from the macromolecule.

Finally this \textem{subtracted curve} is analysed using the so called
\textem{saxsAnalysis} plugin which performs  subsequently Guinier region fitting
using \textem{autorg}, Fourier transform using \textem{datgnom} to obtain
$\rho(r)$ and Porod volume assessement using \textem{datprorod} (those tools
coming from the ATSAS package).
All those information are then directly returned to BsxCube to inform the user
of the success of the experiment and logged into the ISPyBB
database\cite{ispybb} where they can be accessed via an web interface and 
subsequently re-use.

\subsection{Ab initio reconstruction pipeline}

This pipeline is in charge of recontructing the three dimentionnal model based
of the \textem{subtracted curve} and is inspired from a preliminary work
performed at the Diamond Light Source.
It runs multiple instances of \textem{dammif} in parallel (16 by default) to get
multiple models a PDB files. 
After a first
round of outlier rejection based on the goodness of fit (Rf) and the
$sqrt{\chi}$  where the threshold is the mean plus a couple of standard
deviation, only $N$ valid models remains. 
Figure \ref{r} shows the plot which is generated to
explain to the user why certain models are discareded.

All $N$ remaining models are superimposed two by two using
\textem{supcomb}\cite{supcomb}, launching $N*(N-1)/2$ processes in parallel
thanks to the EDNA framework.
The table containing the normalized spatial discrepancy (NSD) is then generated
(see figure \ref{nsd}), explaining which model is the nearest from all other
valid models. This model is called the reference model amd all other valid
models are re-oriented and aligned on the reference one.

All valid models are merged using \textem{damaver}\cite{damaver} and
\textem{damfilt} and finally reprocessed using \textem{damin}\cite{damin}.
This last step is pretty slow an lasts usually for dozens of minutes and is
aborted after half an hour by EDNA as it often runs-away. All results are
uploaded to the ISPyBb database where a visualizer is 


\section{HPLC mode}

A couple of specific pipeline have been developed more recently when the
sample-changer is replaced by a steric exclusion column based  chromatographic
setup (hereafter named HPLC-mode).
In HPLC-mode the eluate of the column is directly connected to the capillary
and the buffer flows though continuously. 2D-diffraction frames are continuously
taken at a frequency around 1 Hertz (the maximum achievable speed with the
current system is 30 Hertz for the detector and around 10 Hertz for the
data-analysis pipeline).
\subsection{HPLC frame}

This pipeline, triggered for every-frame, is similar to the azimuthal
integration pipeline used in sample-changer-mode as it performs the regrouping
of the image and provides the 1D curve.
In addition, this plugin is in charge of defining the reference buffer by
comparing any frame to the first one, assuming the eluate of the first frame
contains pure buffer (using datcmp from the ATSAS package).
Very similar frames (based on a threshold) are merged to the first one to define
the averaged buffer of the experiment.

Any frame very dissimilar (there is a second threshold) to the first/buffer
(check?) is considered as a sample frame hence the buffer signal is subtracted
and Guinier analysis is performed, providing a radius of giration and an
intensity of scattering at q=0.
 
\subsection{HPLC flush}
When the HPLC experiment ends (or is aborted) a specific plugin is launched to
finish the processing of the experiment:
* Merge all 1D diffraction patterns into a 2D dataset containing scattering
intensity in function of scattering vector and time.
* locate peaks, corresponding to the maximum of I0 in Guinier analysis
* merge curves from the same sample to enhance the statistic. All eluate curves
located around a local maxima of I0 with similar Rg are considered
containing the same compound and hence merged togeather.

All those results are saved into a single HDF5 which is uploaded to the ISPyBB
database at the end of the processing. Figure \ref shows the result of an HPLC
experiment where X compounds are separated \ldots.

\section{Offline data analysis}
Describes reprocess mode &
explains why cannot be distributed (licensing, difficulty to install)
 
\section{Hardware used}
Two computers with GPU computing capabilities (Nvidia Quadro 4000) are dedicated
for online data analysis on the biosaxs bemline, they are independent and
feature the same software installation hence providing redondancy (if one
fails).
As the \textem{ab initio} reconstruction pipeline lasts of dozens of minutes
(due to the execution of DAMMIN), it is run on the most powerful computer while 
the azimuthal integration pipeline, which needs the lowest latency, is run on
the other computer.



\section{Conclusion}


\ack{Acknowledgements}
Ricardo Fernandes & Thomas Boeglin: original reprocess 
Peter Boesecke: original azimuthal integration procedure
Olof Svensson: original author of EDNA
Irakli for the original ab-initio pipeline 
Al & Dmitry for the atsas package and support 
Staffan Ohlsson & Matias Guijarro: bliss contact
\bibliographystyle{iucr}
\bibliography{biblio}

% \reference{Author, A. \& Author, B. (1984). \emph{Journal} \textbf{Vol}, 
% first page--last page.}
% \end{references}
% 
%      %-------------------------------------------------------------------------
%      % TABLES AND FIGURES SHOULD BE INSERTED AFTER THE MAIN BODY OF THE TEXT
%      %-------------------------------------------------------------------------
% 
%      % Simple tables should use the tabular environment according to this
%      % model
% 
% \begin{table}
% \caption{Caption to table}
% \begin{tabular}{llcr}      % Alignment for each cell: l=left, c=center, r=right
%  HEADING    & FOR        & EACH       & COLUMN     \\
% \hline
%  entry      & entry      & entry      & entry      \\
%  entry      & entry      & entry      & entry      \\
%  entry      & entry      & entry      & entry      \\
% \end{tabular}
% \end{table}
% 
%      % Postscript figures can be included with multiple figure blocks
% 
% \begin{figure}
% \caption{Caption describing figure.}
% \includegraphics{fig1.ps}
% \end{figure}
% 

\end{document}                
